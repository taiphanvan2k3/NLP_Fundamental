{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning \"\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has good exposure into android development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning ',\n",
       " 'My father is a data scientist and he is nlp expert',\n",
       " 'My sister has good exposure into android development']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "  stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "  punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "  normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learning',\n",
       "  'nlp',\n",
       "  'interesting',\n",
       "  'exciting',\n",
       "  'includes',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'deep',\n",
       "  'learning'],\n",
       " ['father', 'data', 'scientist', 'nlp', 'expert'],\n",
       " ['sister', 'good', 'exposure', 'android', 'development']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in d:\\school\\4thyear\\2ndsemester\\nlp\\labs\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in d:\\school\\4thyear\\2ndsemester\\nlp\\labs\\.venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in d:\\school\\4thyear\\2ndsemester\\nlp\\labs\\.venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\school\\4thyear\\2ndsemester\\nlp\\labs\\.venv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in d:\\school\\4thyear\\2ndsemester\\nlp\\labs\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': 0,\n",
       " 'exciting': 1,\n",
       " 'includes': 2,\n",
       " 'interesting': 3,\n",
       " 'learning': 4,\n",
       " 'machine': 5,\n",
       " 'nlp': 6,\n",
       " 'data': 7,\n",
       " 'expert': 8,\n",
       " 'father': 9,\n",
       " 'scientist': 10,\n",
       " 'android': 11,\n",
       " 'development': 12,\n",
       " 'exposure': 13,\n",
       " 'good': 14,\n",
       " 'sister': 15}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Tao dictionary\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],\n",
       " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting a list of documents (corpus)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# doc2bow(doc) trả về danh sách các từ theo thứ tự tăng dần của word_id thay vì thứ tự xuất hiện ban đầu trong văn bản\n",
    "# Mỗi phần tư tử trong danh sách là một tuple (word_id, word_frequency)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.063*\"father\" + 0.063*\"expert\" + 0.063*\"scientist\" + 0.063*\"data\" + 0.063*\"nlp\" + 0.062*\"includes\" + 0.062*\"machine\" + 0.062*\"exciting\" + 0.062*\"deep\" + 0.062*\"interesting\"'),\n",
       " (1,\n",
       "  '0.173*\"learning\" + 0.121*\"nlp\" + 0.069*\"deep\" + 0.069*\"includes\" + 0.069*\"interesting\" + 0.069*\"machine\" + 0.069*\"exciting\" + 0.069*\"scientist\" + 0.069*\"data\" + 0.069*\"expert\"'),\n",
       " (2,\n",
       "  '0.129*\"sister\" + 0.129*\"good\" + 0.129*\"exposure\" + 0.129*\"development\" + 0.129*\"android\" + 0.032*\"father\" + 0.032*\"scientist\" + 0.032*\"data\" + 0.032*\"expert\" + 0.032*\"nlp\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrix for 3 topics.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "# Results\n",
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.028784046), (1, 0.028392164), (2, 0.9428238)]\n",
      "Topic distribution for the input:\n",
      "Topic 0: 0.0288\n",
      "Topic 1: 0.0284\n",
      "Topic 2: 0.9428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.127*\"learning\" + 0.089*\"and\" + 0.089*\"it\" + 0.051*\"deep\" + 0.051*\"I\" + 0.051*\"very\" + 0.051*\"machine\" + 0.051*\"exciting.\" + 0.051*\"interesting\" + 0.051*\"am\"'),\n",
       " (1,\n",
       "  '0.077*\"My\" + 0.077*\"android\" + 0.077*\"sister\" + 0.077*\"good\" + 0.077*\"development\" + 0.077*\"into\" + 0.077*\"has\" + 0.077*\"exposure\" + 0.019*\"learning\" + 0.019*\"and\"'),\n",
       " (2,\n",
       "  '0.115*\"is\" + 0.066*\"My\" + 0.066*\"expert\" + 0.066*\"he\" + 0.066*\"data\" + 0.066*\"father\" + 0.066*\"a\" + 0.066*\"nlp\" + 0.066*\"scientist\" + 0.065*\"and\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "documents = doc_complete\n",
    "\n",
    "# Tiền xử lý dữ liệu\n",
    "tokenized_documents = [document.split() for document in documents]\n",
    "\n",
    "# Tạo từ điển\n",
    "dictionary = Dictionary(tokenized_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
    "\n",
    "# Huấn luyện mô hình LDA\n",
    "lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Dự đoán chủ đề cho một tài liệu cụ thể\n",
    "new_document = 'My father is a data scientist and he is nlp expert'\n",
    "new_bow = dictionary.doc2bow(new_document.split())\n",
    "topic_distribution = lda_model.get_document_topics(new_bow)\n",
    "\n",
    "# In kết quả\n",
    "print(topic_distribution)\n",
    "print(\"Topic distribution for the input:\")\n",
    "for topic in topic_distribution:\n",
    "    print(f\"Topic {topic[0]}: {topic[1]:.4f}\")\n",
    "\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4685647), (1, 0.27251175), (2, 0.2589236)]\n",
      "Topic distribution for the input:\n",
      "Topic 0: 0.4686\n",
      "Topic 1: 0.2725\n",
      "Topic 2: 0.2589\n",
      "LDA Model Topics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.127*\"learning\" + 0.089*\"and\" + 0.089*\"it\" + 0.051*\"deep\" + 0.051*\"I\" + 0.051*\"very\" + 0.051*\"machine\" + 0.051*\"exciting.\" + 0.051*\"interesting\" + 0.051*\"am\"'),\n",
       " (1,\n",
       "  '0.077*\"My\" + 0.077*\"android\" + 0.077*\"sister\" + 0.077*\"good\" + 0.077*\"development\" + 0.077*\"into\" + 0.077*\"has\" + 0.077*\"exposure\" + 0.019*\"learning\" + 0.019*\"and\"'),\n",
       " (2,\n",
       "  '0.115*\"is\" + 0.066*\"My\" + 0.066*\"expert\" + 0.066*\"he\" + 0.066*\"data\" + 0.066*\"father\" + 0.066*\"a\" + 0.066*\"nlp\" + 0.066*\"scientist\" + 0.065*\"and\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dự đoán chủ đề cho một tài liệu cụ thể\n",
    "new_document = 'I am developing a new android application'\n",
    "new_bow = dictionary.doc2bow(new_document.split())\n",
    "topic_distribution = lda_model.get_document_topics(new_bow)\n",
    "\n",
    "# In kết quả\n",
    "print(topic_distribution)\n",
    "print(\"Topic distribution for the input:\")\n",
    "for topic in topic_distribution:\n",
    "    print(f\"Topic {topic[0]}: {topic[1]:.4f}\")\n",
    "\n",
    "print(\"LDA Model Topics:\")\n",
    "lda_model.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
